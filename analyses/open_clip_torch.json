{
  "project_name": "open_clip_torch",
  "is_a_package": true,
  "self.github_url": "https://github.com/mbzuai-oryx/unimed-clip.git",
  "github_stats": {
    "name": "UniMed-CLIP",
    "language": "Python",
    "commits": 3,
    "newest_commit": "2024-12-16 09:22:45",
    "oldest_commit": "2024-12-16 00:21:21"
  },
  "summary": "UniMed-CLIP is a Python project that implements a robust vision-language model (VLM) specifically designed for diverse medical imaging modalities. It utilizes the UniMed dataset, an expansive open-source compilation of over 5.3 million image-text pairs across six different imaging techniques (X-ray, CT, MRI, Ultrasound, Pathology, and Fundus), to enhance VLM performance in the medical field. The repository includes scripts for dataset preparation, model training, and evaluation, along with pretrained model checkpoints, aimed at promoting advancements and open-source development in medical VLM research.",
  "codebase_size": "592.9 M",
  "total_package_size": "1.1 G",
  "immediate_dependencies": 8,
  "total_number_of_dependencies_in_deps_chain": 46,
  "deepest_file_path": 17,
  "number_of_files": 21319,
  "number_of_tests": 24442,
  "package_tree_analysis_excluding_test_files": {
    "count_of_errors_while_parsing": 1,
    "max_depth": 37,
    "mean_average_depth": 1.19,
    "max_depth_function": "_compat.export_compat",
    "standard_deviation": 0.744,
    "mean_average_depth_excluding_ones": 2.54,
    "standard_deviation_excluding_ones": 1.562
  },
  "package_complexity": {
    "mean_average_complexity": 4.53,
    "max_complexity_function": "validate_https___setuptools_pypa_io_en_latest_references_keywords_html",
    "max_complexity": 335,
    "percent_high_complexity": 0.82
  },
  "error_analysis": {
    "issues": "The code is far too complex for pyflakes to analyze, creating infinite recursion.",
    "errors": "The code is far too complex for pyflakes to analyze, creating infinite recursion."
  }
}